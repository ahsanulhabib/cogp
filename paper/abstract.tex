We propose the collaborative multi-output Gaussian process (GP)  model 
where each output is a weighted combination of sparse latent processes.
 Each latent process has its own set of inducing points that sufficiently encapsulates the process signal and characteristics.
Correlations among the outputs are fostered via sharing of multiple inducing point sets, 
which facilitate  the  application of variational inference and the derivation of 
an evidence lower bound  that decomposes across inputs and outputs. 
We learn all the parameters of the model from data in a  single stochastic optimization framework
that scales to a large number of observations per output and a large number of outputs.
%
We analyze our multi-output model on a toy problem  and evaluate its performance on 
two medium-sized datasets and a large dataset showing the benefits of collaborative learning 
and the superior performance of our approach compared to single output learning and previous 
multi-output GP models.

%% limitation of learning with multiple GPs
%Learning of correlated functions in the popular Gaussian processes (GPs) framework is often performed by mixing latent processes.
%However, when these constituent processes are standard GPs, the applicability of this approach is severely inhibited by their intensive computational and memory demands.
%% our approach
%To address this problem, we propose the collaborative multioutput GPs for multitask regression at very large scale.
%The building blocks of the model are \emph{sparse} processes underpinned by the \emph{inducing variables}.
%These variables captures the sufficient statistics of a pattern (i.e. sparse process) in the data, and dependencies of the outputs are fostered via sharing different sets of inducing points.
%This formulation enables extremely scalable inference of the model via stochastic variational inference.
%Furthermore, all of the model hyperparameters including the inducing inputs can be optimized in the same stochastic approach.
%Experiments on a toy problem and three real-world datasets verify the effectiveness of collaborative learning of sparse processes via the inducing variables promoted by the model.
%They also demonstrate the computational superiority and scalability of the model compared to conventional multiple GPs regression.
