We introduce the collaborative multi-output Gaussian process (GP) model for learning dependent tasks with very large datasets. 
The model fosters task correlations by mixing sparse processes and sharing multiple sets of inducing points. 
This facilitates  the  application of variational inference and the derivation of 
an evidence lower bound  that decomposes across inputs and outputs. 
We learn all the parameters of the model in a  single stochastic optimization framework
that scales to a large number of observations per output and a large number of outputs.
%
We demonstrate our approach on a toy problem, two medium-sized datasets and a large dataset.
The model achieves superior performance compared to single output learning and previous multi-output GP models, confirming the 
benefits of correlating sparsity structure of the outputs via the inducing points.

   %% limitation of learning with multiple GPs
%Learning of correlated functions in the popular Gaussian processes (GPs) framework is often performed by mixing latent processes.
%However, when these constituent processes are standard GPs, the applicability of this approach is severely inhibited by their intensive computational and memory demands.
%% our approach
%To address this problem, we propose the collaborative multi-output GPs for multitask regression at very large scale.
%The building blocks of the model are \emph{sparse} processes underpinned by the \emph{inducing variables}.
%These variables captures the sufficient statistics of a pattern (i.e. sparse process) in the data, and dependencies of the outputs are fostered via sharing different sets of inducing points.
%This formulation enables extremely scalable inference of the model via stochastic variational inference.
%Furthermore, all of the model hyperparameters including the inducing inputs can be optimized in the same stochastic approach.
%Experiments on a toy problem and three real-world datasets verify the effectiveness of collaborative learning of sparse processes via the inducing variables promoted by the model.
%They also demonstrate the computational superiority and scalability of the model compared to conventional multiple GPs regression.
