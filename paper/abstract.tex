% limitation of learning with multiple GPs
Learning of correlated functions in the popular Gaussian processes (GPs) framework is often performed by mixing latent processes.
However, when these constituent processes are standard GPs, the applicability of this approach is severely inhibited by their intensive computational and memory demands.
% our approach
To address this problem, we propose the collaborative multioutput GPs for multitask regression at very large scale.
The building blocks of the model are \emph{sparse} processes underpinned by the \emph{inducing variables}.
These variables captures the sufficient statistics of a pattern (i.e. sparse process) in the data, and dependencies of the outputs are fostered via sharing different sets of inducing points.
This formulation enables extremely scalable inference of the model via stochastic variational inference.
Furthermore, all of the model hyperparameters including the inducing inputs can be optimized in the same stochastic approach.
Experiments on a toy problem and three real-world datasets verify the effectiveness of collaborative learning of sparse processes via the inducing variables promoted by the model.
They also demonstrate the computational superiority and scalability of the model compared to conventional multiple GPs regression.
