\subsection{Appendix 1: Derivatives}
Here we derive the gradients of the lower bound wrt the  hyperparameters.
As a template, consider the lower bound in Hensman et al (as a function of the hyperparameters):
\begin{align}
\nonumber
\calL
=& \log \Normal(\y; \K_{NM} \K_{MM}^{-1} \m, \beta^{-1}\I)
 - \frac{1}{2} \beta \trace \tilde{\K}
 - \frac{1}{2} \beta \trace (\S\K_{MM}^{-1} \K_{MN} \K_{NM} \K_{MM}^{-1}) \\  \nonumber
&- \frac{1}{2} \left( \log |\K_{MM}| + \trace(\K_{MM}^{-1}(\m \m^T + \S)) \right) \\ \nonumber
=& \underbrace{\log \Normal(\y; \A \m, \beta^{-1}\I)}_{\calL_1}
 - \underbrace{\frac{1}{2} \beta \trace (\K_{NN} - \A\K_{MN})}_{\calL_2}
 - \underbrace{\frac{1}{2} \beta \trace (\S\A^T\A)}_{\calL_3} \\  
&- \underbrace{\frac{1}{2} \left( \log |\K_{MM}| + \trace(\K_{MM}^{-1}(\m \m^T + \S)) \right)}_{\calL_4},
\end{align}
where $\A = \K_{NM} \K_{MM}^{-1}$.
Notice that we have re-written the sum of individual terms in matrix form which will make the derivation and also computation easier.

\subsection{Derivative of the Noise Hyperparameter}
The derivative of the noise hyperparameter $\beta$ is easily computed as:
\begin{align}
\deriv{\calL}{\beta} = \frac{N}{2\beta} - \frac{1}{2} (\y - \A\m)^T (\y - \A\m) - \frac{\calL_2}{\beta} - \frac{\calL_3}{\beta}.
\end{align}
\subsection{Derivatives of the Covariance Hyperparameters}  
To simplify the math, we utilize the matrix $\A$ defined above.
Firstly, the derivative of $\A$ wrt a covariance hyperparameter $t$ is given by:
\begin{align}
\deriv{\A}{t} = \left(\deriv{\K_{NM}}{t} - \A \deriv{\K_{MM}}{t}\right)\K_{MM}^{-1}.
\end{align}
The derivatives of $\calL_1, \calL_2, \calL_3 \text{ and } \calL_4$ are thus given by:
\begin{align}
\deriv{\calL_1}{t} &= \beta (\y - \A\m)^T \deriv{\A}{t} \m \\
\deriv{\calL_2}{t} &= \frac{1}{2}\beta \trace \left(\deriv{\K_{NN}}{t} - \A \deriv{\K_{MN}}{t} - \deriv{\A}{t} \K_{MN}\right) \\
\deriv{\calL_3}{t} &= \beta \trace \left(\A \S \deriv{\A^T}{t} \right) \\
\deriv{\calL_4}{t} &= \frac{1}{2}  \trace \left(\K_{MM}^{-1} \deriv{ \K_{MM}}{t}\right) - \frac{1}{2} \trace \left(\K_{MM}^{-1} \deriv{\K_{MM}}{t} \K_{MM}^{-1} (\m \m^T + \S) \right) 
\end{align}
The derivatives are then computed by taking the derivatives of the covariance matrices $\K_{NN} (\text{the diagonal only}), \K_{NM} \text{ and }  \K_{MM}$, hence the covariance function, wrt the hyperparameters. 

\subsection{Derivatives of the Inducing Inputs}
To compute the derivatives of $\calL$ wrt the inducing inputs, first notice that $\Z = \{\z_m\}_{m=1}^M$ are also parameters of the covariance matrices $\K_{NM}$ and $\K_{MM}$.
Hence the derivative wrt a single dimension of an inducing input, i.e. $z_{mj}$, is the same as that of $\deriv{ \calL}{t}$.

%Since $MD$ parameters are needed for the inducing inputs, it appears that the derivatives of all inducing inputs would require $\calO(MD \times M^3)$ in computation.
%However, this complexity can actually be reduced to $\calO(DM^3)$ using the following lemma. \\
%
%\noindent \textbf{Lemma} Let $A, B$ be two matrices of size $N \times M$ and $M \times N$, respectively. Furthermore, $B$ has the property that only one of its rows or columns is non-zero. The complexity of $\trace(AB)$ is only $\calO(N)$. \\
%
%\noindent \textbf{Proof} Let $m <= M$ be the non-zero row of $B$. We have
%\begin{align}
%\trace (AB) = \sum_{i=1}^N \sum_{j=1}^M A_{ij} B_{ji} = \sum_{i=1}^N A_{im} B_{mi},
%\end{align}
%which clearly takes $\calO(N)$. It is easy to see that the lemma also holds when $B$ is symmetric and only one of its row and the corresponding column is non-zero. \\
%
%\noindent To exploit the property in the Lemma, we re-write $\frac{d \calL_1}{dt}, \frac{d \calL_2}{dt}, \frac{d \calL_3}{dt}, \frac{d \calL_4}{dt}$ by expanding $\frac{d\A}{dt}$ (here $t = z_{mj}$):

\noindent We re-write $\deriv{\calL_1}{t}, \deriv{\calL_2}{t}, \deriv{ \calL_3}{t}, \deriv{\calL_4}{t}$ by expanding $\deriv{\A}{t}$ (here $t = z_{mj}$):
\begin{align}
\nonumber
% dL1
\deriv{\calL_1}{t}
 &= \beta \trace (\y - \A\m)^T \left(\deriv{\K_{NM}}{t} -  \A \deriv{ \K_{MM}}{t}\right)\K_{MM}^{-1} \m \\
&= \beta \trace \K_{MM}^{-1} \m (\y - \A\m)^T \deriv{\K_{NM}}{t} 
-\beta \trace \K_{MM}^{-1} \m (\y - \A\m)^T \A \deriv{\K_{MM}}{t} \\
%dL2
\deriv{\calL_2}{t}
&= - \beta \trace \A^T \deriv{\K_{NM}}{t}
 + \frac{1}{2} \beta \trace \A^T \A \deriv{\K_{MM}}{t}  \\
% dL3
\deriv{\calL_3}{t}
&= \beta \trace \K_{MM}^{-1} \S \A^T \deriv{\K_{NM}}{t}
 - \beta \trace \K_{MM}^{-1} \S \A^T \A \deriv{\K_{MM}}{t}\\
% dL4
\deriv{\calL_4}{t}
 &= \frac{1}{2}  \trace \K_{MM}^{-1} \deriv{\K_{MM}}{t}
  - \frac{1}{2} \trace \K_{MM}^{-1} (\m \m^T + \S) \K_{MM}^{-1} \deriv{\K_{MM}}{t} 
\end{align}
From the above equations we get,
\begin{align}
\deriv{\calL}{t} = \trace \D_1 \deriv{\K_{NM}}{t} + \trace \D_2 \deriv{ \K_{MM}}{t},
\end{align}
where 
\begin{align}
\D_1 =& \beta \K_{MM}^{-1} \m (\y - \A\m)^T
 + \beta \A^T
 - \beta \K_{MM}^{-1} \S \A^T \\ \nonumber
\D_2 =& -\beta \trace \K_{MM}^{-1} \m (\y - \A\m)^T \A
 - \frac{1}{2} \beta \A^T \A
  + \beta \K_{MM}^{-1} \S \A^T \A	 \\ 
  &-\frac{1}{2} \K_{MM}^{-1} + \frac{1}{2} \K_{MM}^{-1} (\m \m^T + \S) \K_{MM}^{-1}
\end{align}
Notice that $\D_1$ and $\D_2$ can be pre-computed with a cost of $\calO(M^3)$ (or $\calO(N_bM^2)$ if the minibatch size $N_b > M$).
The computational cost of taking derivatives of $MD$ inducing parameters is thus $\calO(M^3 + MDM) = \calO(M^3)$ as the cost of the two $\trace$ operators is $\calO(M)$ due to the fact that only $\calO(M)$ elements of $\deriv{\K_{MM}}{t}$ or $\deriv{\K_{NM}}{t}$ are non-zero.
This fact can be further exploited to perform vectorized operation, for e.g. in Matlab, such that the iteration over all inducing inputs can be avoided.

\subsection{Appendix 2: Gaussian Identity}
Let $\y$, $\g = \{\g_j\}_{j=1}^q$, and $\h$ be random variables with multivariate Gaussian distributions: 
$p(\y | \g, \h) = \Normal(\y; \sum_{j=1}^Q \W_j \g_j + \W \h, \beta^{-1} \I)$, $p(\g_j) = \Normal(\g_j; \m_j, \S_j)$, and $p(\h) = \Normal(\h; \m, \S)$.
The following identity is important in deriving the evidence lower bound:
\begin{align}
\nonumber
\int \log p(\y | \g, \h) \prod_{j=1}^q p(\g_j) p(\h) \der \g \der \h
=& \log \Normal(\y; \sum_{j=1}^q \W_j \m_j + \W \m, \beta^{-1} \I) - \frac{1}{2} \beta \trace \W^T \W \S \\
\label{eq:identity}
&- \frac{1}{2} \beta \trace \sum_{j=1}^q \W_j^T \W_j \S_j.
\end{align}
The identity can be proved by using this fact: 
\begin{align}
\nonumber
\int (\W\x - \Mu)^T \BigSigma^{-1} &(\W \x - \Mu) \Normal(\x; \m, \S) \der \x \\
\nonumber
&= (\Mu - \W\m)^T \BigSigma^{-1} (\Mu - \W\m) + \trace \W^T \BigSigma^{-1} \W \S.
\end{align}

