We have presented scalable multi-output GPs for learning of correlated functions.
The formulation around the inducing variables was shown to be conducive to effective and scalable joint learning under sparsity. 
We note that although our large scale experiments were done with over 40,000 observations -- the largest publicly available multi-output dataset found, the model can easily handle much bigger datasets.

%Trung: commented this out for space
%We discuss several interesting extensions to this work.
%First, consider the model for the single GP case, i.e., $P = 1$. By using $Q > 1$ sparse processes, the GP can now have multiple set of inducing variables.
%These can be made local to capture varying characteristics of the latent process in different  input regions, for e.g. similar to the 
%work by \citet{nguyen2014fast}, leading to scalable GPs with non-stationary properties. 

%A further extension is to replace the constant mixing weights in the model with input-dependent coefficients as in the Gaussian process regression networks framework \citep{wilson-et-al-icml-12} to accommodate adaptive dependencies among the outputs.
%We leave  these extensions to future work.