%\subsection{Variational Inference in Sparse GPs Revisited}
%
%In this section we review inference in sparse GPs (as presented in Titsias).
%The posterior in an augmented model is $p(\f, \u | \y) = p(\f | \u, \y) p(\u | \y)$.
%The key property of such augmented GPs  is the notion of \textit{sufficient statistics}: given the inducing points $\u$, the latent values $\f$ are independent with any other set of latent values (e.g. the test set).
%In the optimal setting when $\u$ is the sufficient statistics of $\f$, it should hold that $p(\f | \u, \y) = p(\f | \u)$ as $\y$ is only the noisy version of $\f$.
%This leads to choosing a variational approximation of the posterior which factorizes as $q(\f, \u | \y) = p(\f | \u) q(\u | \y)$.
%Since the conditional $p(\f | \u)$ is known, variational inference becomes learning an optimal posterior $q(\u | \y)$ only. \\
%
We approximate the posterior over the latent variables $\vec{g}, \vec{h}, \vec{u}, \vec{v}$ given observations $\vec{y}$ using variational inference.
In section \ref{sec:variationalLowerBound} we derive a lower bound of the marginal likelihood which has the key property of factorizing over the data points and outputs. Section \ref{sec:svi}  takes advantage of this factorization to 
derive stochastic variational inference, allowing the model to scale to very large datasets.
Section \ref{sec:complexity} compares the complexity of the model with previous multi-output methods.

\subsection{VARIATIONAL LOWER BOUND \label{sec:variationalLowerBound}}
\newcommand{\ug}{\u_g}
\newcommand{\uh}{\u^h}
\newcommand{\mgj}{\m_j}
\newcommand{\mhi}{\m^h_i}
\newcommand{\Sgj}{\S_j}
\newcommand{\Shi}{\S^h_i}
Our inference task is to find the posterior $p(\g, \h, \u, \v | \y)$, for which we use variational inference \citep{jordan-variational-99}. 
We first observe that the true posterior distribution can be written as:
\begin{align}
p(\g, \h, \u, \v | \y) = p(\g | \u, \y) p(\h | \v, \y) p(\u, \v | \y).
\end{align}
Here we recall the modeling assumption that each set of the inducing variables is the \emph{sufficient statistics} of the corresponding latent process. 
This motivates replacing the true posteriors over $\g$ and $\h$ with their conditional distributions given the inducing variables, leading to a distribution of the form:
\begin{align}
q(\g, \h, \u, \v | \y)
= p(\g|\u) p(\h|\v) q(\u,\v),
\end{align}
with
\begin{align}
q(\u,\v) &= \prod_{j=1}^Q \underbrace{\Normal(\u_j; \mgj, \Sgj)}_{q(\u_j)} \prod_{i=1}^P  \underbrace{\Normal(\v_i; \mhi, \Shi)}_{q(\v_i)}.
\end{align}
This technique has been used by \citet{titsias2009variational} and \citet{hensmangaussian} 
to derive variational inference algorithms for the single output case.
Since the conditionals $p(\g | \u)$ and $p(\h | \v)$ are known (Equations \eqref{eq:gu} and \eqref{eq:hv}), 
we only need to 
learn $q(\u, \v)$ so as to minimize the divergence between the approximate posterior and
the true posteriors.
The quality of approximation depends entirely on the posterior over the inducing variables, thus underlining their pivotal role in the model as previously discussed.

\noindent
To find the best distribution $q(\u, \v)$, we  optimize the evidence lower bound (ELBO) of the log marginal:
\begin{align}
%\nonumber
%\log p(\y) \ge& \int q(\u, \v) \log \frac{p(\y | \u, \v) p(\u, \v)}{q(\u, \v)} \der \u \der \v \\
%\nonumber
%=& \int q(\u, \v) \log p(\y | \u, \v)  \der \u \der \v 
%+ \int q(\u, \v) \log \frac{p(\u, \v)}{q(\u, \v)} \der \u \der \v \\
\nonumber
&\log p(\y) \ge \int q(\u, \v) \log p(\y | \u, \v)  \der \u \der \v \\
&- \sum_{j=1}^Q \KL[q(\u_j) || p(\u_j)] - \sum_{i=1}^P \KL[q(\v_i) || p(\v_i)],
\label{eq:elbo}
\end{align}
which is derived using Jensen's inequality and the fact that both of $q(\u, \v)$ and $p(\u, \v)$ fully factorize.
Since $q(\u_j), q(\v_i), p(\u_j), p(\v_i)$ are all multivariate Gaussian distributions, the KL divergence terms are analytically tractable.
To compute the expected likelihood term in the ELBO we first see that
\begin{align}
\nonumber
\log \text{ } &p(\y | \u, \v)
% &= \log \Eb{p(\y | \g, \h)}_{p(\g,\h | \u, \v)} \\
% \nonumber
\ge \Eb{\log p(\y | \g, \h)}_{p(\g,\h | \u, \v)}  \\
&= \sum_{i=1}^P \sum_{n=1}^N \Eb{\log p(y_{in} | \g_n, h_{in}) }_{p(\g | \u) p(\h_i | \v_i)} 
\end{align}
where $\g_n = \{g_{jn} = (\g_j)_n\}_{j=1}^Q$.
The inequality is due to Jensen's inequality and the equality is due to the fact that the likelihood fully factorizes.
\newcommand{\Ahi}{\A^h_i}
\newcommand{\Zi}{\Z_i}

The ELBO can be computed by first solving for the individual expectations 
%$l_{in} \define \Eb{\log p(y_{in} | \g_n, h_{in}) }_{p(\g_n | \u) p(\h_in | \v_i)}$ 
$ \Eb{\log p(y_{in} | \g_n, h_{in}) }$ over ${p(\g | \u) p(\h_i | \v_i)}$ and 
then substituting these into Equation \eqref{eq:elbo} (see the supplementary material for details). 
Hence the resulting lower bound is given by:
\begin{align}
\nonumber
%&\log p(\y)
 & \calL = 
\sum_{i,n}
\bigg( \log  \Normal(y_{in}; \tilde{\mu}_{in}, \beta_i^{-1})
          - \frac{1}{2} \beta_i \sum_{j=1}^Q w_{ij}^2 \tilde{k}_{jnn} \\ \nonumber
         &- \frac{1}{2} \beta_i \tilde{k}^h_{inn}
         - \frac{1}{2} \beta_i \sum_{j=1}^Q \trace w_{ij}^2 \S_j \mat{\Lambda}_{jn} - \beta_i \frac{1}{2} \trace \S^h_i \mat{\Lambda}_{in} 
\bigg) \\
\nonumber
& - \sum_{j=1}^Q \bigg( \frac{1}{2} \log |\K_{jzz} \S_j^{-1}| + \frac{1}{2} \trace \K_{jzz}^{-1}  \left( \m_j \m_j^T + \S_j \right)\bigg) \\
\nonumber
& - \sum_{i=1}^P  \bigg( \frac{1}{2} \log |\K_{izz}(\Shi)^{-1}| \\
& \quad \qquad + \frac{1}{2} \trace \K_{izz}^{-1} \left( \mhi (\mhi)^T   +   \Shi \right) \bigg)  \text{,}
  \label{eq:L}
\end{align}
%
where $\K_{jzz} = k(\Zj, \Zj)$, $\K_{izz} = k(\Zhi, \Zhi)$, and:
\begin{align}
\tilde{\mu}_{in}
%&= \sum_{j=1}^Q w_{ij} k(\x_n, \Zj)k(\Zj,\Zj)^{-1}\m_j + k(\x_n, \Zhi)k(\Zhi,\Zhi)^{-1}\mhi \\
&= \sum_{j=1}^Q w_{ij} \A_j(n,:) \m_j + \Ahi(n,:) \mhi \text{,}\\
\mat{\Lambda}_{jn}
%&= k(\Zj,\Zj)^{-1} k(\Zj, \x_n) k(\x_n, \Zj) k(\Zj,\Zj)^{-1} \\
&= \A_j(n,:)^T \A_j(n,:)  \text{,}\\
\mat{\Lambda}_{in}
%&= k(\Zhi,\Zhi)^{-1} k(\Zhi, \x_n) k(\x_n, \Zhi) k(\Zhi,\Zhi)^{-1}
&= \Ahi(n,:)^T \Ahi(n,:)  \text{,}
\end{align}
with $\tilde{k}_{jnn} = (\tilde{\K}_j)_{nn}$; 
$\tilde{k}^h_{inn} = (\tilde{\K}^h_i)_{nn}$; 
$\mu_{jn} = (\Mu_j)_n$;
 $\mu^h_{in} = (\Mu^h_i)_n$; 
and we have defined the auxiliary matrices $\A_j = k(\X,\Zj) \K_{jzz}^{-1}$ and $\Ahi = k(\X_i,\Zhi)\K_{izz}^{-1}$ and 
used $\A_j(n,:)$ to denote the $n$-th row vector of $\A_j$. 
Notice that this ELBO clearly generalizes the bound for standard GP regression derived in \cite{hensmangaussian}, which can be recovered by setting $P = Q = 1$, $w_{ij} = 1$ and $h_i(\x) = 0$.

The novelty of the  variational lower bound in Equation  \eqref{eq:L} is that it decomposes across both inputs and outputs.
This enables the use of stochastic optimization methods, which 
allow the model to handle  very large datasets for which existing GP-based multi-output models are simply 
impractical.  
%
\subsection{STOCHASTIC VARIATIONAL INFERENCE \label{sec:svi}}
\newcommand{\oi}{\vec{o}_i}
% some notation

So far in the description of the model and inference  we have
 implicitly assumed that every output has full observations at all inputs $\X$.
To discern where learning occurs for each output, we make the missing data scenario more explicit.
Specifically, each output $i$ can have observations at a different set of inputs $\X_i$. 
We shall use $\oi$ to denote the indices of $\X_i$ (in the set $\X$) and use the indexing operator $\B(\oi)$ to select the rows corresponding to $\oi$ from any arbitrary matrix $\B$.
We also overload $\y_i$ as the observed targets of output $i$. 
%
\subsubsection{Learning the Parameters of the Variational Distribution}
%------------------------------------------
% derivatives of q(u_j)
\newcommand{\Lgj}{\calL^g_j}
\newcommand{\ynoj}{\y_i^{\backslash j}}
\newcommand{\Kjzz}{\mat{K}_{jzz}}
 We can obtain the derivatives of the ELBO in Equation  \eqref{eq:L}
  wrt the variational parameters for optimization.
The derivatives of $\calL$ wrt the parameters of $q(\u_j)$ are given by:
\begin{align}
\deriv{\calL}{\m_j} 
=& \sum_{i=1}^P \beta_i w_{ij} \A_j(\oi)^T \ynoj \\
\nonumber
&- \bigg[\Kjzz^{-1} + \sum_{i=1}^P \beta_i w_{ij}^2 \A_j(\oi)^T \A_j(\oi) \bigg] \m_j \text{,} \\
\deriv{\calL}{\S_j} 
=& \frac{1}{2} \S_j^{-1} - \frac{1}{2} \bigg[ \Kjzz^{-1} + \sum_{i=1}^P \beta_i w_{ij}^2 \A_j(\oi)^T \A_j(\oi) \bigg],
\end{align}
where $\y_i^{\backslash j} = \y_i - \Ahi(\oi) \mhi - \sum_{j' \neq j} w_{ij'} \A_{j'}(\oi) \m_{j'}$.
%\noindent As a function of the parameters of $q(\u_j)$, the lowerbound $\calL$ is:
%\begin{align}
%\nonumber
%\calL^g_j \define&
% \sum_{i=1}^P \log \Normal(\y_i; \sum_{j=1}^Q w_{ij} \A_j(\oi,:) \m_j + \Ahi \mhi, \beta_i^{-1} \I)  \\
% \nonumber
% &- \frac{1}{2} \sum_{i=1}^P \bigg(\beta_i \trace w_{ij}^2 \tilde{\K}_j(\oi,\oi) 
% + \beta_i \trace w_{ij}^2 \S_j \A_j(\oi,:)^T \A_j(\oi,:) \bigg)
% \\
% &- \frac{1}{2} \log |k(\Zj,\Zj) \S_j^{-1}| -\frac{1}{2} \trace k(\Zj,\Zj)^{-1} (\m_j \m_j^T + \S_j) ,
%\end{align}
%where $\A_j = k(\X,\Zj)k(\Zj,\Zj)^{-1}$, which gives $\A_j(\oi,:) = k(\X_i,\Zj) k(\Zj,\Zj)^{-1}$, and  
%$\Ahi = k(\X_i,\Zhi)k(\Zhi,\Zhi)^{-1}$. \\

%-------------------------------------------
%  derivatives of q(v_i)
\newcommand{\Lhi}{\calL^h_i}
\newcommand{\Kizz}{\mat{K}_{izz}}
\noindent The derivatives of $\calL$ wrt the parameters of $q(\v_i)$ are given by:
\newcommand{\ynoh}{\y_i^{\backslash h}}
\begin{align}
\nonumber
\deriv{\calL}{\mhi}
= & \beta_i \Ahi(\oi)^T \ynoh  \\
&- \bigg[\Kizz^{-1} +  \beta_i \Ahi(\oi)^T \Ahi(\oi) \bigg] \m_i \text{,} \\
\deriv{\calL}{\Shi} 
=& \frac{1}{2} \S_i^{-1} - \frac{1}{2} \bigg[ \Kizz^{-1} + \beta_i \Ahi(\oi)^T \Ahi(\oi) \bigg] ,
\end{align}
where $\ynoh = \y_i - \sum_{j=1}^Q w_{ij} \A_j(\oi,:) \m_j$.
%\noindent As a function of the parameters of $q(\v_i)$, the lower bound $\calL$ is:
%\begin{align}
%\nonumber
%\Lhi \define&
% \log \Normal(\y_i; \sum_{j=1}^Q w_{ij} \A_j(\oi,:) \m_j + \Ahi \mhi, \beta_i^{-1} \I)
% - \frac{1}{2} \beta_i \trace \tilde{\K}^h_i(\oi,\oi)
% - \frac{1}{2} \beta_i \trace \Shi (\Ahi)^T \Ahi
% \\
%  &- \frac{1}{2} \log |k(\Zhi,\Zhi) (\Shi)^{-1}| -\frac{1}{2} \trace k(\Zhi,\Zhi)^{-1} (\mhi (\mhi)^T + \Shi) ,
%\end{align}

% comment on computation
\noindent It can be seen that the derivatives of the parameters of $q(\v_i)$ only involve the observations of the output $i$.
The derivatives of the parameters of $q(\u_j)$ involve the observations of all outputs but is a sum of contributions from individual outputs.
Computation of the derivatives can therefore be easily distributed or parallelized.
%Therefore, computation of the derivatives can be distributed or parallelized easily.
%This property further enhances the scalability of the model, allowing it to handle big data problems with a very large number of inputs and outputs.

Since the optimal distributions $q(\u_j)$ and $q(\v_i)$ are in the exponential family, it is more convenient to use stochastic variational inference \citep{hensman2012fast,hensmangaussian} to perform update of their canonical parameters.
This works by taking a step of length $l$ in the direction of the natural gradient approximated by mini-batches of the data.
For instance, consider $q(\u_j)$ whose canonical parameters are $\Phi_1 = \S_j^{-1} \m_j$ and $\Phi_2 = -\frac{1}{2}\S_j^{-1}$.
Their stochastic update equations at time $t + 1$ are given by:
\begin{align}
\nonumber
& \Phi_{1(t+1)} = \S_{j(t)}^{-1} \m_{j(t)}  \\
 & \quad \qquad +  l \bigg(\sum_{i=1}^P \beta_i w_{ij} \A_j(\oi)^T \ynoj - \S_{j(t)}^{-1} \m_{j(t)} \bigg) \\
& \Phi_{2(t+1)} = -\frac{1}{2} \S_{j(t)}^{-1} + l \bigg(\frac{1}{2} \S_{j(t)}^{-1} - \frac{1}{2} \mat{\Lambda} \bigg),
\end{align}
\normalsize
where $\mat{\Lambda} = \Kjzz^{-1} + \sum_{i=1}^P \beta_i w_{ij}^2 \A_j(\oi)^T \A_j(\oi)$.
%
\subsubsection{Inducing Inputs and Hyper-parameters \label{sec:hyperparameters}}
To learn the hyperparameters, which in this model include the mixing weights, the covariance hyperparameters of the latent processes, and the noise precision of each output, we follow standard practice in GP inference.
For this model this involves taking derivatives of the ELBO and applying standard stochastic gradient descent in alternative steps with the variational parameters, much like a variational EM algorithm. 
The derivatives are given in the supplementary material.

Learning of the inducing inputs, which was not considered in the single output case in \citet{hensmangaussian}, is also possible in our stochastic optimization approach.
In the supplementary material, we show that 
%with the help of vectorized operations, 
the cost of computing the derivatives of the lower bound wrt the inducing inputs is subdominant to that  of updating the variational parameters.
%can be computed with a cost \emph{independent of the input dimensionality}.
This makes optimizing the inducing locations a practical option that can be important, for example, in high-dimensional problems where fixing the inducing inputs may hinder the performance of the model.
Indeed, our experiments on a large scale multi-output problem show that automatic learning of the inducing inputs can lead to significant performance gain with little overhead in computation. 
%
\subsection{COMPLEXITY ANALYSIS \label{sec:complexity}}
In this section we analyze the complexity of the model and compare it to existing multi-output approaches.
For consistency, we first unify common notations used for all models.
We use $P$ as the number of outputs, $N$ as the number of inputs, $Q$ as the number of shared latent processes, $M_t$ as the \emph{total} number of inducing inputs.
It is worth noting that $M_t = (P + Q) \times M$ in our COGP model, assuming that each sparse process has equal number of inducing points. 
Also, COGP have $P$ additional individual processes, one for each output.

The complexity of COGP can be read off by inspecting the ELBO  in Equation \eqref{eq:L}, with a key observation that it contains 
a sum over the outputs as well as over the inputs.
This means a mini-batch containing a small subset of the inputs and outputs can be used for stochastic optimization.
Technically, the cost is $\calO(M^3)$ or $\calO(N_bM^2)$, where $N_b$ is the size of the mini-batches, depending on which is larger between $M$ and $N_b$. 
In practice, we may use $N_b > M$ as a large batch size (e.g.~$N_b = 1000$) helps reduce stochasticity of the optimization.
However, here we use $\calO(M^3)$  for easier comparison with other models whose time and storage demands are 
given in Table \ref{tab:complexity}. 
We see that COGP has 
a computational  complexity that is independent of the size of the inputs and outputs, 
thus is the only method capable of handling large scale problems.
%
\setlength{\tabcolsep}{4pt}
\begin{table}[t]
\caption{Comparison of the time and storage complexity of approximate inference of  multi-output GP models. 
A\&W, 2009 refers to \citet{alvarez-lawrence-nips-08}. COGP is the only method with complexity \emph{independent} of the
 number of inputs $N$ and outputs $P$, thus it can scale to very large datasets.}
\label{tab:complexity}
\begin{center}
\small
\begin{tabular}{lcc}
\toprule
\textbf{METHOD} & \textbf{TIME} & \textbf{STORAGE} \\ \hline
COGP, this paper  & $\calO(M^3)$ & $\calO(M^2)$ \\
SLFM, \citep{teh-et-al-aistats-05} & $\calO(QNM_t^{2})$ & $\calO(QNM_t)$ \\
MTGP, \citep{bonilla-et-al-nips-08} & $\calO(PNM_t^2)$ & $\calO(PNM_t)$\\ 
CGP-FITC (A\&W, 2009)& $\calO(PNM_t^2)$ & $\calO(PNM_t)$ \\
GPRN, \citep{wilson-et-al-icml-12} & $\calO(PQN^3)$ & $\calO(PQN^2)$ \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\subsection{PREDICTION}
The predictive distribution of the $i$-th output for a test input $\x_*$ is given by:
\begin{align}
p(\fstar | \y, \x_*) = \Normal(\fstar; \sum_{j=1}^Q w_{ij} \mu_{j*} + \mu^h_{i*}, w_{ij}^2 s_{j*} + s^h_{i*}), 
\end{align}
where $\mu_{j*}$ and $s_{j*}$ are the mean and variance of the prediction for $g_{j*} = g_j(\x_*)$, i.e. $p(g_{j*} | \y, \x_*) = \Normal(g_{j*}; \mu_{j*}, s_{j*})$.
Likewise, $\mu^h_{i*}$ and $s^h_{i*}$ are the mean and variance of the prediction for $h_{i*} = h_i(\x_*)$, $p(h_{i*} | \y, \x_*) = \Normal(h_{i*}; \mu^h_{i*}, s^h_{i*})$.
These predictive means and variances are given by: 
\begin{align}
\mu_{j*} &= \vec{k}_{j*z} \K_{jzz}^{-1} \m_j  \text{,}\\ 
s_{j*} &= k_{j**} - \vec{k}_{j*z} \left(\K_{jzz}^{-1} - \K_{jzz}^{-1} \S_j \K_{jzz}^{-1} \right) \vec{k}_{j*z}^T  \text{,}\\
\mu^h_{i*} &= \vec{k}_{i*z} \K_{izz}^{-1} \mhi \text{,} \\
s^h_{i*} &= k_{i**} - \vec{k}_{i*z} \left(\K_{izz}^{-1} - \K_{izz}^{-1} \S_i \K_{izz}^{-1} \right) \vec{k}_{i*z}^T \text{,}
\end{align}
where $k_{j**} = k_j(\x_*, \x_*)$, $k_{i**} = k^h_i(\x_*, \x_*)$,  $\vec{k}_{j*z}$ is the covariance between $\x_*$ and $\Zj$, and $\vec{k}_{i*z}$ is the covariance between $\x_*$ and $\Zhi$.

%\noindent Also as a consequence of sufficient statistics, the approximate prediction for test targets $\vfstar$ at test inputs $\X_*$ is
%\begin{align}
%\nonumber
%p(\vfstar | \y, \X_*) &= \int p(\vfstar | \f, \u, \X_*) q(\f, \u | \y) \der \f \der \u \\
%\nonumber
%&= \int p(\vfstar | \u) q(\u| \y) p(\f | \u) \der \f \der \u \\ \nonumber
%&= \int p(\vfstar | \u) q(\u| \y) \der \u \\
%\label{eq:sorprediction}
%&= \Normal(\vfstar; \bs{\mu_*},\vec{s_*})
%\end{align}
%where,
%\begin{align}
%\nonumber
%\bs{\mu_*} &= \K_{*z} \K_{zz}^{-1}\m \\ 
%\nonumber
%\S_* &= \K_{**} - \K_{*z} \left(\K_{zz}^{-1} - \K_{zz}^{-1} \S \K_{zz}^{-1} \right) \K_{*z}^T.
%\end{align}
% Here $\K_{*z}$ is the covariance matrix between test and inducing inputs, $\K_{zz}$ is the covariance matrix of the inducing inputs.
 
%\begin{align}
%p(\fstar | \y, \x_*) = \int \Normal(\fstar; \sum_{j=1}^Q w_j g_{j*} + h_{i*}, 0) p(\g_* | \y, \x_*) p(h_{i*} | \y, \x_*) \der \g_* \der h_{i*},
%\end{align}
