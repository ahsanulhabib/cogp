% Gaussian processes are good for bayesian regression, Very flexible in multi-output settings, for e.g. bonilla, alvarez, wilson
Gaussian process models  \citep[GPs,][]{rasmussen-williams-book}  are a popular choice in 
Bayesian regression due to their ability to capture complex dependencies and non-linearities in data.
In particular, when having multiple outputs or tasks they have proved effective in modeling 
the dependencies between the tasks  and  they have been shown to outperform competitive baselines and 
single output learners  \citep{bonilla-et-al-nips-08,teh-et-al-aistats-05,alvarez-lawrence-nips-08,wilson-et-al-icml-12}.
% However they are costly
However, the prohibitive cost of performing exact inference in  GP models severely hinders 
their application to  large scale multi-output problems. For example, na\"{i}ve inference 
in a fully coupled Gaussian process model over $P$  outputs and $N$ data-points can have 
a complexity of $\calO(N^3P^3)$  and $\calO(N^2P^2)$ in time and memory, respectively.


% Motivating example is good
A motivating  example of a large scale multi-output application 
is the tracking of  movements of a robot arm using 2 or more joint torques. 
If one of the robot motors malfunctions and fails to record a torque, data collected from the 
other motors may be used to infer the missing torque values.
However, taking 100 measurements per second already results in a total of over 40,000 data points 
per torque in just 7 minutes.
Clearly this problem is well beyond the capabilities  of conventional multiple output GPs.
Building  multi-output GP models that can learn correlated tasks at the scale of 
these types of problems  is thus the main focus of this paper.

% Tranditional approaches in single output setting, based e..g. on inducing points are not good enough 
In the single output setting previous attempts to scale up  GP inference 
resort to approximate inference. Most  approximation techniques 
 can be understood 
within a single probabilistic framework that uses a set of \emph{inducing points}  in order to 
obtain an approximate process (or a low-rank approximate covariance) over which inference can be 
performed more efficiently \citep{quinonero2005unifying}. These models have been referred 
to in the literature as sparse models. 
Nevertheless, straightforward application of such approximate techniques will yield a computational 
cost of at least $\calO(PNM^2)$ in time and  $\calO(PNM)$ in memory, where 
$M$ is the number of inducing points. This high complexity still prevents us from applying GP models 
to  large scale multi-output problems.

% Useful observations
In this work we approach the challenge of building scalable multi-output Gaussian process models  
based on the following observations.
Firstly, \emph{inducing variables} are the key catalyst
for achieving sparsity and dealing with large scale problems in Gaussian process models.
In particular, they capture the \emph{sufficient statistics} of a dataset allowing the construction of sparse 
processes that can approximate arbitrarily well the exact GP model \citep{titsias2009variational}.
Secondly, the use of global latent variables (such as the inducing points) allows us to 
induce dependencies in a highly correlated model efficiently. 
This observation is exploited in \cite{hensmangaussian} for single output GP regression models 
where, by explicitly representing a distribution over the inducing variables, stochastic variational inference 
can be used to work with millions of data points.
Finally, the key to multi-output and multi-task learning is to model dependencies between the outputs  
based on realistic assumptions of what can be shared across the tasks. It turns out that 
sharing  ``sparsity structure'' can not only be a reasonable assumption but also a crucial component
when modeling dependencies between different related tasks.


% Our approach starts from mixing latent process
% having inducing points for sparsity
% sharing inducing points across the latent process
Based on these observations, we propose the collaborative multi-output Gaussian Process (COGP) model 
where each output is a weighted combination of sparse latent processes.
 Each latent process has its own set of inducing points that sufficiently encapsulates the process signal and characteristics.
Correlations among the outputs are fostered via sharing of multiple inducing point sets 
which in turn create a medium through which joint inference happens efficiently.
% exploit stochastic variational inference
In particular, we derive a variational lower bound of the model evidence that decomposes across 
inputs and outputs. 
This decomposition makes possible the application of stochastic variational inference, 
thus allowing the model to handle a large number of observations per output and a large number of outputs.
Furthermore, learning of all  the  parameters in the model, including
kernel hyperparameters and inducing inputs, can be done in a single stochastic optimization framework.

We analyze our multi-out model on a toy problem  where the inducing variables are shown to be 
conducive to the sharing of information between two related tasks. 
Additionally, we  evaluate  our model on two moderate-sized datasets in which we show that it
can outperform previous non-scalable multi-output approaches as well as single output baselines.
Finally, on a  large scale experiment regarding the learning of robot inverse dynamics we show 
the substantial benefits of collaborative learning provided by our model.
  
% Experiments and summary of results
%The rest of the paper is organized as follows.
%We give description of the model in Section \ref{sec:model}, followed by the inference procedure in Section \ref{sec:inference}. 
%Section \ref{sec:experiments} presents empirical evaluation.
%We conclude with some discussion in Section \ref{sec:discussion}.

% Some citations that can be brought back somewhere
\paragraph{Related work.}
Most GP-based multi-output models create correlated outputs by mixing a set of independent latent processes.
The mixing can be a linear combination with fixed coefficients \citep[see e.g.][]{teh-et-al-aistats-05,bonilla-et-al-nips-08}. This is known in the geostatistics community as  the ``linear model of coregionalization'' \citep{goovaerts1997geostatistics}.
Such models may also be reformulated in a common Bayesian framework, for example by placing a spike and slab prior over the coefficients \citep{titsias2011spike}.
More complex dependencies can be induced by using 
 input-dependent coefficients \citep{wilson-et-al-icml-12,nguyen2013efficient}  or convolving processes \citep{boyle-frean-nips-05,alvarez-lawrence-nips-08,alvarez2010efficient}.
 
While we also use the mixing construction, the key difference in our model is the role of inducing variables.
In particular, when used in previous models to reduce the computational costs \citep[see e.g.][]{alvarez-lawrence-nips-08,alvarez2010efficient}, the inducing points are integrated out or collapsed. 
In contrast, our model maintains an explicit representation of the posterior over the inducing variables that is learned using data from all outputs.
This explicit representation  facilitates scalable learning in a similar fashion to 
the approach in \citet{hensmangaussian}, making it applicable to very large datasets. 











