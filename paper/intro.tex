% paper on multioutput so this should appear first in the introduction
Gaussian processes \citep[GPs,][]{rasmussen-williams-book} are perhaps the dominant approach for learning functions in non-parametric Bayesian regression.
Unfortunately, the prohibitive complexity when performing exact inference in the GP framework severely hinders its application to real world problems. 
In particular, when learning $P$ multiple correlated functions over $N$ data points, the complexity is $\calO(N^3P^3)$ in computation and $\calO(N^2P^2)$ in memory.
An example of a real-world application where multioutput regression may be useful is the tracking of  movements of a robot arm using 2 or more joint torques. 
If one of the robot motors malfunctions and fails to record a torque, data collected from the other motors may be used to infer the missing torque values.
However, taking 100 measurements per second already results in a total of over 40,000 data points per torque in just 7 minutes.
Clearly this problem is well beyond the capability of conventional multiple output GPs.
Building scalable multioutput GPs model that can learn correlated tasks at the scale of the application just considered and much larger is thus the main focus of this paper.

% traditional approach 
The traditional approach in multiple GP regression has been to mix independent latent processes to generate correlated functions.
The mixing can be a weighted linear combinations with fixed \citep{teh-et-al-aistats-05,bonilla-et-al-nips-08} or input-dependent \citep{wilson-et-al-icml-12,nguyen2013efficient} coefficients or convolution of processes \citep{boyle-frean-nips-05,alvarez-lawrence-nips-08}.
The correlations of the outputs are induced through sharing of these base processes.
% sparse approximation technique and limitatio
Sparse approximate techniques such as the informative vector machine (IVM) \cite{lawrence2002fast} or low-rank approximations based around the inducing points framework \citep{quinonero2005unifying},
are then employed to reduce the computational and storage demands of the full models.
Nevertheless, most of these models have complexity of at least $\calO(PNM^2)$ in computation and $\calO(PNM)$ in memory, where $M$ is the number of inducing points.
thus prevents their applications to the  large scale problems we are targeting.

% we approach from different perspective 
In this work, we approach the challenge of building scalable multioutput Gaussian processes based on several observations.
First, \emph{sparsity} is an inevitable assumption in very large scale GPs, as evident by the vast amount of research on sparse approximations in the single output setting. % can cite ref here 
Second, the \emph{inducing variables} are the key catalyst for effective and scalable learning under sparsity.
In particular, they capture the \emph{sufficient statistics} of a dataset and lead to optimization of sparse processes that converge to the exact GP model \citep{titsias2009variational}.
This is further exploited in \cite{hensmangaussian} where by explicitly representing a distribution over the inducing variables, stochastic variational inference can be used to work with millions of data points.

% can we do joint learning of multiple output under the sparsity assumption (meantioned above) for 
Motivated by these observations, we propose the collaborative multioutput GPs (COGP) model where each output is a weighted combination of sparse processes. 
Accompanying each process is its own set of inducing points that sufficiently encapsulates the process signals and characteristics.
Correlations among the outputs are fostered via sharing of multiple inducing sets which in turn create a medium through which joint inference happens.

% contribution from inference for large scale
This formulation around inducing points and sparse processes enables scalable inference of the model.
In particular, we derive a variational lower bound of the model evidence that factorizes across not only the outputs but also the inputs.
This factorization makes possible the application of stochastic variational inference, thus allowing the model to handle large scale problems, in terms of both of the number of inputs and outputs.
Also, learning of all of the  hyperparameters, including the inducing inputs, can be done in a consistent stochastic optimization framework.

The effectiveness of the model is first verified on a toy problem where the inducing variables are shown to be conducive to the sharing of information between two related tasks. 
Further experiments are conducted on two moderate-sized datasets in which the model shows better performance compared to other baselines.
Our large scale experiment confirms the substantial benefits of collaborative learning brought about by the model, 
thus opening up new opportunities for improving the performance over learning with single sparse process. 
  
% Experiments and summary of results
The rest of the paper is organized as follows.
We give description of the model in Section \ref{sec:model}, followed by the inference procedure in Section \ref{sec:inference}. 
Section \ref{sec:experiments} presents empirical evaluation.
We conclude with some discussion in Section \ref{sec:discussion}.

%Gaussian processes (GPs) \citep{rasmussen-williams-book} are perhaps the prior of choice in nonparametric Bayesian regression not only in the standard setting but also in multioutput learning \citep{} .
% it is beauty but also a beast
%Unfortunately, given a dataset of size $N$, exact inference for GP regression requires $\calO(N^3)$ for computation and $\calO(N^2)$ for memory, which can only be applied to problems in non-realistic settings.
% cite titsias
%Consequently, various approximate methods \citep{lawrence2002fast,seeger2003bayesian,seeger2003fast, smola2001sparse,snelson2006sparse,williams2001using} have been proposed to overcome this limitation.
%Taking insights from the seminal work of \citep{quinonero2005unifying}, most of these approximations can be formulated around the \emph{inducing points}.
%These come from the same latent function that generates the data and can be viewed as representing its sufficient statistics \citep{titsias2009variational}.
%Using these approaches, the complexity reduces to $\bigO(NB^2)$ in computation and $\bigO(NB)$ in memory where $B \ll N$ is the number of inducing points.
%Recent development in stochastic variational inference for GPs \citep[GPSVI,][]{hensmangaussian} has led to further reduction of these costs to $\bigO(B^3)$ and $\bigO(B^2)$ respectively.

% While the important advance by GPSVI has made possible the application of GPs to datasets of millions of data points, the assumption that the statistics of the data is sufficiently captured by a, typically small, set of inducing variables might be too strong and limiting.
%This is addressed in \citep{nguyen2014fast} by using local sets of inducing points, as opposed to a global set, such that each local inducing sets sufficiently support a different local region of the input space.
%Yet another approach to this challenge is multi-task / multi-output learning, which is the focus of this paper.
%Specifically we pose the question: \emph{under the sparsity constraint of individual outputs}, can multiple output learning be effective?
%We believe this to be an interesting and  important research problem toward scalable multioutput GP models.
%Here we emphasize the word \emph{sparsity}, as it has been evident from the vast literature on GP approximations that  sparsity is the impeccable assumption to deal with large scale datasets.
%When learning things together, the hope is that information from different tasks can be used collaboratively to compensate for the sparsity enforcement to the individual outputs.
 
