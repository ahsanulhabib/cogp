Before diving into technical details of the model specification, 
we discuss the modeling philosophy behind our collaborative multi-output Gaussian processes.
To learn the outputs jointly, we need a mechanism through which information can be transferred among the outputs.
This is achieved in the model by allowing the outputs to share multiple sets of inducing variables, 
each of which captures a different pattern common to the outputs.
These variables  play a double pivotal role in the model: they collaboratively share  information 
across the outputs and  provide \emph{sufficient statistics}  so as to induce sparse processes. 

\newcommand{\Zj}{\Z_j}
\newcommand{\Zhi}{\Z^h_i}
Consider the joint regression of $P$ tasks with inputs $\X = \{\x_n \in \calR^D\}_{n=1}^N$ and outputs $\y = \{\y_i\}_{i=1}^P$ where $\y_i = \{y_{in}\}_{n=1}^N$.
We  model each output as a weighted combination of $Q$ shared latent functions $\{ g_{j}\}_{j=1}^{Q}$, plus an individual latent 
function $\{ h_{i}\}_{i=1}^{P}$ unique to that output for greater flexibility.
The $Q$ shared functions have independent Gaussian process priors  $g_j(\x) \sim \GP(0, k_j(\cdot,\cdot))$. %, with $j= 1, \ldots Q$.
Similarly, each individual function of an output also has GP prior, i.e.~$h_i(\x) \sim \GP(0, k^h_i(\cdot,\cdot))$. %with $i=1 ,\ldots P$.


As we want to sparsify these processes, 
we introduce the set of \emph{shared inducing variables} $\u_j$ for  each $g_j(\x)$, i.e. $\u_j$ contains the values of 
$g_j(\x)$ at the inducing inputs $\Z_j$. Likewise, we have 
individual inducing variables corresponding to each  $h_i(\x)$,  which we denote with with $\v_i$ and their 
corresponding inducing inputs $\Zhi$.
%
The inducing inputs lie in the same space as the inputs $\X$.
% which should be apparent since their corresponding values come from the processes defined on the input space.
For convenience, we  assume all processes have the same number of inducing points, $M$.
However we emphasize that this is not
 imposed in practice.

% information is transfered via the inducing variables

We denote the collective variables: $\g = \{\g_j\}$, $\h = \{\h_i \}$, $\u = \{\u_j\}$, $\v = \{\v_i\}$, $\Z = \{\Zj\}$, and $\Z^h = \{\Zhi \}$ where $\g_j = \{g_j(\x_n)\}$, $\h_i = \{h_i(\x_n)\}$. 
Note that we reserve subscript $i$ for indexing the outputs and their corresponding individual processes ($i = 1 \hdots P$), $j$ for the shared latent processes ($j = 1 \hdots Q$), and $n$ for the inputs ($n = 1 \hdots N$).
%
\subsection{PRIOR MODEL}
From the definition of the GPs and the independence of the processes, 
the \emph{prior} of the multi-output model can be written as:
\begin{align}
\label{eq:gu}
p(\g | \u) &= \prod_{j=1}^Q p(\g_j | \u_j) = \prod_{j=1}^Q \Normal(\g_j; \BigMu_j, \tilde{\K}_j )\\
\label{eq:u}
p(\u) &= \prod_{j=1}^Q p(\u_j) = \prod_{j=1}^Q \Normal(\u_j; \vec{0}, k(\Zj, \Zj)) \\
\label{eq:hv}
p(\h | \v) &= \prod_{i=1}^P p(\h_i | \v_i) = \prod_{i=1}^P \Normal(\h_i; \BigMu^h_i, \tilde{\K}^h_i)\\
\label{eq:v}
p(\v) &= \prod_{i=1}^P p(\v_i) = \prod_{i=1}^P \Normal(\v_i; \vec{0}, k(\Zhi, \Zhi)),
\end{align}
where the corresponding means and covariances of the Gaussians are given by:
\begin{align}
 \BigMu_j &= k(\X,\Zj)k(\Zj,\Zj)^{-1}\u_j \\
\BigMu^h_i &= k(\X,\Zhi)k(\Zhi,\Zhi)^{-1}\v_i \\
\tilde{\K}_j &= k_j(\X,\X) - k(\X,\Zj)k(\Zj,\Zj)^{-1}k(\Zj,\X) \\
\tilde{\K}^h_i &= k^h_i(\X,\X) - k(\X,\Zhi)k(\Zhi,\Zhi)^{-1}k(\Zhi,\X).
\end{align}
In the equations and hereafter, we omit the subscripts $j,h,i$ from the kernels $k_j(\cdot,\cdot)$ and $k^h_i(\cdot,\cdot)$ when it is clear from the parameters inside the parentheses which covariance function is in action.

Equations \eqref{eq:u} and \eqref{eq:v} follow directly from the properties of GPs, while the expressions for $p(\g|\u)$ and $p(\h|\v)$ (Equations  \eqref{eq:gu} and \eqref{eq:hv}) come from the conditionals of the multivariate Gaussian distributions.
Instead of writing the joint priors $p(\g,\u)$ and $p(\h,\v)$, the above equivalent equations are given to emphasize the \emph{sufficient statistics} role of $\u$ and $\v$ in the model.
Here we use sufficient statistics in the same sense as in \citet{titsias2009variational}, i.e. 
for any sparse process (say $g_j$), any other set of function values is independent with $\g_j$ given the inducing variables $\u_j$.
%
\subsection{LIKELIHOOD MODEL}
As mentioned above, we assume that  observations for each output 
are  (noisy) linear combinations of the $Q$ latent functions $g_{j}(\vec{x})$ plus
an independent function $h_{i}(\vec{x})$. Hence we have that the likelihood 
with standard iid Gaussian noise is given by:
\begin{align}
p(\y | \g, \h ) = \prod_{i=1}^P \prod_{n=1}^N \Normal( y_{in} ; \sum_{j=1}^Q w_{ij} g_j(\x_n) + h_i(\x_n), \beta_i^{-1}),
\end{align}
where $w_{ij}$ are the corresponding weights and $\beta_i$ is the precision of each Gaussian.
%which says that $y_{in}$ is a linear combination of the latent functions with weight $w_{ij}$ for the $j$th shared latent process plus a contribution from the individual process $h_i$.
As the latent values $\g$ are specified conditioned on the inducing variables $\u$, this construction implies that each output is a weighted combination of the inducing values.
We note that if $\u$ and $\v$ are marginalized out, we obtain the semiparametric latent factor model \citep{teh-et-al-aistats-05}.
However, doing so is against the purpose of our model which encourages sharing of outputs via the inducing variables.
Furthermore, as we shall see in the next section, explicit representation of these variables is fundamental to scalable inference of the model.

%\textbf{Augmented sparse GPs}
%Toward scalable modeling, we replace standard GPs with sparse GPs augmented with \textit{different} set of inducing inputs.
%This adds much flexibility to the model as each of the shared process $g_j(\x)$ can model a different pattern in the data with its own covariance function and inducing inputs. The roles of $g_j(\x)$ and $h_i(\x)$ can be quite different, so it is necessary that each has its own inducing inputs.
%Furthermore, $g_j(\x)$ can be seen as a \textit{global} function operating on the entire input space (of all output dimensions), while each $h_i(\x)$ operates only on the inputs of the $i$-th output, which can be a subspace of the input.
