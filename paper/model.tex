We describe a multiple-output regression model.
Suppose that we have inputs $\X = \{\x_n\}_{n=1}^N$ and outputs $\y = \{\y_n\}_{n=1}^N$, with $\x_n \in \calR^D$ and $\y_n \in \calR^P$, i.e. the output is $P$-dimensional.
For ease of exposition, we assume that there are no missing values in any output dimension.
The most dominant approach in GP-based multiple output regression is to compose the different outputs as some \textit{linear} mixing (combination) of some basic processes (random functions).
Our model is also underpinned by this composition approach.

\noindent Consider the semiparametric latent factor models (SLFM) \cite{teh-et-al-aistats-05}.
It assumes $Q$ basic independent processes, $g_j(\x) \sim \GP(0, k(\x,\x';\vectheta_j))$, where $\vectheta_j$ is the hyperparameters of the covariance function of $g_j(\x)$.
Each (latent) output is a weighted combination of these basic processes, i.e.
\begin{align}
f_i(\x) = \sum_{j=1}^Q w_{ij} g_j(\x), \quad i = 1 \hdots P,
\end{align}
In \cite{seeger2005semiparametric}, an extra degree of freedom is given by allowing each output $i$ to have its own process $h_i(\x) \sim \GP(0, k(\x,\x'; \vectheta^h_i)$ leading to
\begin{align}
f_i(\x) = \sum_{j=1}^Q w_{ij} g_j(\x) + h_i(\x), 
\end{align}
We emphasize that, although $k$ denotes covariance functions in general, each independent process can have its own covariance function.

\noindent 
\textbf{Augmented sparse GPs}
Toward scalable modeling, we replace standard GPs with sparse GPs augmented with \textit{different} set of inducing inputs.
This adds much flexibility to the model as each of the shared process $g_j(\x)$ can model a different pattern in the data with its own covariance function and inducing inputs. The roles of $g_j(\x)$ and $h_i(\x)$ can be quite different, so it is necessary that each has its own inducing inputs.
Furthermore, $g_j(\x)$ can be seen as a \textit{global} function operating on the entire input space (of all output dimensions), while each $h_i(\x)$ operates only on the inputs of the $i$-th output, which can be a subspace of the input.

%However, to really address the issue of scalability, this model is built upon sparse GPs from the ground up.

\subsection{Prior and Likelihood}
In this section we specify the prior and likelihood of this model. 
\newcommand{\Zj}{\Z_j}
\newcommand{\Zhi}{\Z^h_i}
Let the set of inducing inputs for $g_j(\x)$ and $h_i(\x)$ be $\Z_j$ and $\Z^h_i$, and the corresponding inducing points $\u_j$ and $\v_i$, respectively.
We denote the collective variables: $\g = \{\g_j\}$, $\h = \{\h_i \}$, $\u = \{\u_j\}$, $\v = \{\v_i\}$, $\Z = \{\Zj\}$, and $\Z^h = \{\Zhi \}$ where $\g_j = \{g_j(\x_n)\}$, $\h_i = \{h_i(\x_n)\}$. 
The subscripts are $i = 1 \hdots P$, $j = 1 \hdots Q$, and $n = 1 \hdots N$.

\noindent
\textbf{Prior}
The prior of the augmented model is given by:
\begin{align}
p(\g | \u) &= \prod_{j=1}^Q p(\g_j | \u_j) = \prod_{j=1}^Q \Normal(\g_j; \BigMu_j, \tilde{\K}_j )\\
p(\u) &= \prod_{j=1}^Q p(\u_j) = \prod_{j=1}^Q \Normal(\u_j; \vec{0}, k(\Zj, \Zj)) \\
p(\h | \v) &= \prod_{i=1}^P p(\h_i | \v_i) = \prod_{i=1}^P \Normal(\h_i; \BigMu^h_i, \tilde{\K}^h_i)\\
p(\v) &= \prod_{i=1}^P p(\v_i) = \prod_{i=1}^P \Normal(\v_i; \vec{0}, k(\Zhi, \Zhi)),
\end{align}
where $\BigMu_j = k(\X,\Zj)k(\Zj,\Zj)^{-1}\u$ and $\tilde{\K}_j = k_j(\X,\X) - k(\X,\Zj)k(\Zj,\Zj)^{-1}k(\Zj,\X)$ and $\BigMu^h_i, \tilde{\K}^h_i$ are similarly defined.
To avoid notational clutter, we omit the subscripts $j,h,i$ from the kernels $k_j(\cdot,\cdot)$ and $k^h_i(\cdot,\cdot)$ when it is clear from the parameters inside the parentheses which covariance function is in action. 

\noindent
\textbf{Likelihood}
The likelihood as usual follows the standard iid Gaussian likelihood,
\begin{align}
p(\y | \g, \h ) = \prod_{i=1}^P p( \y_i ; \g, \h_i) = \prod_{i=1}^P \Normal( \y_i ; \sum_{j=1}^Q w_{ij} \g_j + \h_i, \beta_i^{-1} \I).
\end{align}

