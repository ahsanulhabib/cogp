% Using the factorized kernel to fit the data generated by some other processes or models
% (such as convolved gps, gprn, etc). Then generate some functions from the
% approximated kernels.
% 1. Generates sample from true MTGP model
% 2. Selects data-points for training
% 3. Plots training data
% 4. Sets parameters for learning (including initialization) 
% 5. Learns hyperparameters with minimize function
% 6. Makes predictions at all points on all tasks
% 7. Plots the predictions
function mtgp_prediction()

% IMPLEMENTATION NOTE:
% - use help covFunctions to see how parameters are stored in covfunctions,
% particularly for covSEard. Note that the length-scales matrix is L =
% [ell_1^2 ... ell_D^2] and the hyperparameters it expect is [log(ell_1)
% ... log(ell_D)]. Care should be taken when convert back and forth between
% the values in log-scale and normal scale
% - mtgp pass in the wrong order of hyperparameters for covSEard so should
% be careful to re-arrange the hyperparameters for correctness.

Q = 4;
covfunc_x = 'covSEard';
axes = [0, 2, 0, 10];
color = ['b', 'g', 'm', 'r', 'c'];
markersize = 1.6;

basedir = 'data'; basefile = 'gprnx';
[xtrain, ytrain, xtest, ytest] = load_data(basedir, basefile);
x = xtrain;
y = ytrain'; % N x Q matrix where each row is the output of M tasks
[N D]= size(x); % N = number of sample per tasks
n = N*Q;        % n = N * M = total number of output points
y = y(:);
v = repmat((1:Q),N,1); % why Q x N? probably for Kf x Kx
ind_kf_train = v(:); % indices to tasks
v = repmat((1:N)',1,Q);
ind_kx_train = v(:); % indices to input space data-points

% 2. Set up data points for training
% ntrain = n in our case
nx = ones(n,1); % Number of observations on each task-input point
ytrain = y; xtrain = x;

% 4. Settings for learning
irank = Q;                         % Full rank

% Notice that Kf is a positive semi-definite matrix that specifies the
% inter-task similaries. Here Kf is parameterized with M*(M+1)/2
% parameters. Had we use the same kernel function, such as gaussian, with
% same scale parameter then number of parameter would be 1. On the other
% hand, if use different scales for each pair of fq, fq' then we need the
% same numbers to parametrize Kf; hence can learn the elements in Kf
% directly rather than learning the scales.

nlf = irank*(2*Q - irank +1)/2;    % Number of parameters for Lf
theta_lf0 = init_Kf(Q,irank);      % params for L, the cholesky decomposition of Kf
theta_kx0 =  init_kx(D);           % params for Kx
theta_sigma0 = init_sigma(Q);      % noise variance
logtheta_all0 = [theta_lf0; theta_kx0; theta_sigma0];

% 5. We learn the hyper-parameters here
%    Optimize wrt all parameters except signal variance as full variance
%    is explained by Kf
deriv_range = ([1:nlf,nlf+2:length(logtheta_all0)])';
logtheta0 = logtheta_all0(deriv_range);
niter = 100; % setting for minimize function: number of function evaluations
[logtheta nl] = minimize(logtheta0,'learn_mtgp',niter, logtheta_all0, ...
			 covfunc_x, xtrain, ytrain,...
			 Q, irank, nx, ind_kf_train, ind_kx_train, deriv_range);
%
% Update whole vector of parameters with learned ones
logtheta_all0(deriv_range) = logtheta;
theta_kf_learnt = logtheta_all0(1:nlf);
Lf_learnt = vec2lowtri_inchol(theta_kf_learnt,Q,irank); % lower triangular decomposition of log(Kf)
Kf = Lf_learnt * Lf_learnt'
% D is known already
ltheta_x = eval(feval(covfunc_x)); % Number of parameters of theta_x
theta_kx_learnt = logtheta_all0(nlf+1 : nlf+ltheta_x)
theta_sigma_learnt = logtheta_all0(nlf+ltheta_x+1:end);
sigma2n = exp(2*theta_sigma_learnt)
Sigma2n = diag(exp(2*theta_sigma_learnt));
% Keep in mind that theta_kx_learnt is in wrong order for covSEard...
generateSamples(Kf, theta_kx_learnt(2:end), Sigma2n);

%xtest = (-2:0.05:2)';
figure(21); hold on;
Xtest = (0:0.01:2)';
for task=1:Q
  Ntest = size(Xtest,1);
  [alpha, Kf, L, Kxstar] = alpha_mtgp(logtheta_all0, covfunc_x, xtrain, ytrain,...
				    Q, irank, nx, ind_kf_train, ...
				    ind_kx_train, Xtest);
  all_Kxstar = Kxstar(ind_kx_train,:);
  Kf_task = Kf(ind_kf_train,task);
  Ypred = (repmat(Kf_task,1,Ntest).*all_Kxstar)'*alpha;
  plot(Xtest, Ypred, color(task));
%   disp(['Mean absoluate error for output ' num2str(task)]);
  mean(abs((ytest(task,:)' - Ypred)))
end
title('Predictions by factorized kernel');
legend(num2str((1:Q)'));
%axis(axes);

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
function theta_kx0 = init_kx(D)
theta_kx0 = [log(1); log(rand(D,1))];
end

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
function theta_lf0 = init_Kf(M,irank)
Kf0 = eye(M);                 % Init to diagonal matrix (No task correlations)
Lf0 = chol(Kf0)';
theta_lf0 = lowtri2vec_inchol(Lf0,M,irank);
end

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
function theta_sigma0 = init_sigma(M)
theta_sigma0 =  (1e-7)*rand(M,1);  
end

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Generate sample from a multi-task setting
% theta_x is the length-scale matrix use in k(x,x), theta_x =
% [ell_1^2,...,ell_D^2]
function generateSamples(Kf, theta_kx, Sigma2n)
  X = (0:0.01:2)';
  nSamples = size(X, 1);

  Kx = feval(covfunc_x, [theta_kx; log(1)], X);
  K = kron(Kf, Kx);
  if ~isempty(Sigma2n)
    temp = repmat((1:Q),nSamples,1); % why Q x N? probably for Kf x Kx
    ind_kf = temp(:); % indices to tasks
    K = K + Sigma2n(ind_kf, ind_kf); % add noise
  end

  % Sampling from Gaussian N(0, K) gives an utility function 
  %rng(30, 'twister');  
  fqx = gsamp(zeros(size(K, 1), 1), K, 1); % fqx has size 1xQN (1 row)
  fqsample = reshape(fqx, nSamples, Q)';
  
  % plot the utility functions of all Q users
  figure(22); hold on;
  for q=1:Q
    plot(X, fqsample(q, :), color(q));
  end
  title('Utility functions generated by learned factorized kernel');
  legend(num2str((1:Q)'));
  %axis(axes);
end

end
